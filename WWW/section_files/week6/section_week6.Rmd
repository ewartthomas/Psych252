---
title: "Section: Week 6 - Contrasts & Mediation"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
---

## Quiz tactics

* Report all relevant statistics (in APA format) -- whether significant or not!
* Explain what the stats mean! What is the direction of the effect (e.g., as X increased, Y also increased)? What might be causing those results?
* If describing data, feel free to use bullet points and note the stats + interpretation conscisely if running out of time
* Use R when appropriate to save time, and given extra time write down what you can for credit if you make mistakes (e.g., the code you entered into R; the equations you'd use)
* If you get stuck on a question, move on and come back to it later! 
* It's better to write down something rather than leaving an answer blank. Even if you just write down the equations you would use (or the test more generally), rather than calculating everything.
* If working on homeworks in groups, make sure fully understand each question; try each question on your own before consulting with other people so that you get practice generating how to solve a given problem!

## A note on interactions

Check out this [Nature article](http://www.nature.com/neuro/journal/v14/n9/full/nn.2886.html) about the importance for testing formally for interactions. It's not enough to say that Group A had a significant positive effect of X and Y, but Group B didn't; you need to formally test for an interaction between group and X on Y. This is a big error, that is suprisingly made quite often (even in prominent journals like *Science* and *Nature*!).

## Homework Question A 

```{r echo=FALSE}
#Load in libraries
rm(list=ls())
library(ggplot2)
library(scatterplot3d)
library(effects)
library(tidyr)
library(GGally)
library(psych)
theme_set(theme_bw(base_size = 18)) 
```

```{r echo=FALSE}
#sDefine functions

plot_dataframe <- function(data, color_var=NULL, cols=NULL){
  if(missing(cols)){
    cols=c(1:length(data))
    }
  
  if(missing(color_var)){
    ggpairs(data[, cols], 
            upper = list(continuous = "smooth", 
                         combo = "box"),
            lower = list(continuous = "cor", 
                         combo = "facethist"))
    }else{
      ggpairs(data[, cols], 
              upper = list(continuous = "smooth", 
                           combo = "box"),
              lower = list(continuous = "cor", 
                           combo = "facethist"),
              colour = color_var)
      }
  }
```

### Define your research question

*Do family-friendly programs in organizations (e.g., flexible work hours, on-site childcare, etc.) have an effect on employee satisfaction?*

Let's clear out our working space, load in the data, etc.
```{r workspace}
d=read.csv("http://www.stanford.edu/class/psych252/data/families.csv")
```

### Look at the data

Taking a look at the data.

```{r data}
str(d)
summary(d)
```

Now we know that we have:
`N = 68` companies in our sample

**Measures**

`famprog:` the amount of family-friendly programs from (1) Nothing at all to (9) Amazing family-friendliness

`empsatis:` the average rating of employee satisfaction from (1) Extremely unsatisfied to (7) Extremely satisfied

`perfam:` the percentage of employees with families in the organization from 0% to 100%

**Describe the data**

First let's summarize the data
```{r}
describe(d)
```

Now, to look at the relationships between variables, we'll visualize the data w/plots.

```{r}
plot_dataframe(d)
```

### Consider main effects

**Main effects: Does the number of programs affect employee satisfaction? Is the percentage of families who use the programs correlated with employee satisfaction?**

First, we could run a simple regression where `famprog`, the number of family-friendly programs, predicts `empsatis`, employee satisfaction:
```{r simple reg}
# Stats
e_by_famprog = lm(empsatis ~ scale(famprog, scale=FALSE), data = d)
summary(e_by_famprog)
```

Seems like the answer is yes, but we have a weak (i.e., **marginal**) effect. Can we do something to describe the data better? Taking a look at our plot, what do we think might be going on?

### Consider interactions (and/or quadratic effects)

We could use our other predictor `perfam`, and look for an interaction with family programs on employee satisfaction:
```{r interaction continuous}
e_by_famprogXperfam = lm(empsatis ~ scale(famprog, scale=FALSE) * 
                           scale(perfam, scale=FALSE), data = d)
summary(e_by_famprogXperfam)

# additive model
e_by_famprogplusperfam = lm(empsatis ~ scale(famprog, scale=FALSE) +
                           scale(perfam, scale=FALSE), data = d)

# compare 2 models
anova(e_by_famprogplusperfam, e_by_famprogXperfam)
```

### Visualize the interactions

```{r}
d$perfam_nMS[ d$perfam < median(d$perfam) ] = 'Low Percentage Families'
d$perfam_nMS[ d$perfam > median(d$perfam) ] = 'High Percentage Families'
d$perfam_nMS = as.factor( d$perfam_nMS )
str(d)

# Begin plotting
ggplot(na.omit(d), aes(famprog, empsatis, 
              color = perfam_nMS, 
              fill=perfam_nMS)) +
  stat_smooth(method=lm) + 
  geom_point()
```


### Write up results

**Make sure to:**

* Use APA format!
* Explain what the results (e.g., p < 0.05) mean; what is the direction of the effect? What have we learned from this analysis?
* Give a brief causal story about why this might be the case

#### Interpret the interaction/higher order effects

**Does the percentage of employees with families impact the effect of family programs on employee satisfaction?**

When examining whether the percentage of employees with families impacts the effect of family programs on employee satisfaction, we observed a significant interaction between the number of family friendly programs and the percentage of employees with families on employee satisfaction, $b=0.007, t(64)=2.06, p < 0.05$. More specifically, for companies that had a low percentage of employees with families, an increase in the number of family friendly programs did not really affect employee satistfaction. However, for companies that had a high percentage of employees with families, the more family friendly programs they had, the higher employee satisfaction was. This is likely the case because employees with families could make better use of the family friendly programs, etc.

### Further explore the interaction

**Interpret the interaction by examining the effect of family programs on employee satisfaction for companies who have the average number of employees with families**

Revisit our centered lm:

```{r} 
at_mean = lm(empsatis ~ scale(famprog, scale = F) * 
               scale(perfam, scale = F), data = d)
summary(at_mean)

# compare to:
# summary(lm(empsatis ~ scale(famprog, scale = F), data = d))
```

Because we've centered both variables (0 = mean), the "simple effects" reported here are at the mean of each variable.

So for the effect of family programs on employee satisfaction for companies who have the average number of employees with families, $b = .065, t(64)=1.84, p = .07$. Thus, for companies who have the average number of employees with families, as they increase the number of family friendly programs, employee satisfaction trends toward increasing.

So, what do we mean below by the "main effect" of family programs on employee satisfaction? Lesson: don't rely on these terms "simple" and "main" effect to make yourself understood. Be clear in describing your interpretation and exactly what it means. This is also helpful in coming up with a good mechanistic interpretation.

**Interpret the interaction by examining the effect of family programs on employee satisfaction for companies at +1SD and -1SD of mean % who use programs**

Say we want to know about companies that have a lot of employees with families who use these programs (+1SD). We calculate this by literally subtracting 1SD from the centered value of `perfam`.

```{r sd above}
at_plus1SD = lm(empsatis ~ scale(famprog, scale = F) * 
                  I(scale(perfam, scale = F)-sd(perfam)), data = d)
summary(at_plus1SD)
```

So for the number of family programs at **+1SD above the mean** of the percentage of families who use programs, the effect of family programs on employee satisfaction is $b = .14, t(64) = 2.79, p < .01.$ Interpret this!

*Remember that we subtract one SD to describe the effect at one SD above the mean! You're subtracting these levels from your centered variable, so in the case of -1 SD, so +1 SD*

Now, for companies with few families who use these programs, we'll look at **1SD below the mean** of the percentage of families who use these programs

```{r sd below}
at_minus1SD = lm(empsatis ~ scale(famprog, scale = F) * 
                   I(scale(perfam, scale = F)+sd(perfam)), data = d)
summary(at_minus1SD)
```

$b = -.01, t(64)=-0.23, p=0.82$ Interpret this!

**Answer**: List the simple effects at each level of `perfam`.

For companies where few families use the family programs (i.e., a low percentage of employees have families), the number of programs does not affect employee satisfaction, `b = -.01, t(64) = -.23, p >.80.` However, for companies where a lot of families use the family programs (i.e., a high percentage of employees have families), the number of family programs is associated with higher employee satisfaction, `b = .14, t(64) = 2.70, p = .007.`


#### Visualize the interaction/simple effects 

We can also plot these specific results to visualize the continuous interaction (specifically the simple effects of famprog at +/- 1SD of perfam) using the regression equations from above.

At the mean of perfam: $\hat{y} = 3.94 + 0.07\text{famprog}$
At -1SD perfam: $\hat{y} = 4.12 - 0.12\text{famprog}$
At +1SD perfam: $\hat{y} = 3.77 + 0.14\text{famprog}$

```{r fig.width=5, fig.height=5, warning=FALSE}
ggplot(d, 
       aes(x=scale(famprog, scale=F), 
           y=empsatis)) +  # Adding color for mentill
  geom_point(alpha=0.7) +  
  # effect of famprog on empsatis @mean perfam
  geom_abline(aes(intercept=at_mean$coefficients[1], 
                  slope=at_mean$coefficients[2]), colour='black', size=2, alpha=.9) +
  # effect of famprog on empsatis -1 SD perfam
  geom_abline(aes(intercept=at_minus1SD$coefficients[1], 
                  slope=at_minus1SD$coefficients[2]), colour='red', size=2, alpha=.9) +
  # effect of famprog on empsatis +1 SD perfam
  geom_abline(aes(intercept=at_plus1SD$coefficients[1], 
                  slope=at_plus1SD$coefficients[2]), colour='dodgerblue', size=2, alpha=.9) 
```


Alternately, get the plot just using the original model:
```{r echo=FALSE, fig.width=5, fig.height=5, warning=FALSE}
d$famprog_c = as.numeric(scale(d$famprog, scale=F))
d$perfam_c = as.numeric(scale(d$perfam, scale=F))
modelv = lm(empsatis ~  perfam_c * famprog_c, data=d)
summary(modelv)
with(d, plot(famprog_c,empsatis,type="n")) # Plot without dots
xv<-seq(-4,4,1)
for(i in -1:1){
lines(xv,predict(modelv,
                 list(famprog_c=xv,
                      perfam_c=rep(i*sd(d$perfam_c),
                                     length(xv)))),
      lty=2+i,col=10+i)
}
legend("topleft", title="self_image",c("1 SD below","At mean","1 SD above"), lty = 1:3,col=9:11, cex=1)
```

### Return to the main/simple effects

**Do family-friendly programs improve employee satisfaction overall?**

Since we've centered our variables, we can essentially interpret the lower order terms as main effects. You can compute the main effects with separate models too, e.g., `lm(empsatis ~ scale(famprog, scale=FALSE) * , data = d)`.

Answer: Yes, there is a marginal positive main effect of the number of family programs on employee satisfaction, and we would report the b, t, df, and p, and then give a short interpretation of the results (e.g., "as __ increases, __ also increases.")
`b = .07, t(64) = 1.84, p = .07`

### Conclusion

**What do you conclude? Write out the story. Remember to use the appropriate numbers to make the story as useful as possible!**

```{r descriptives}
mean(d$perfam)
mean(d$perfam)+sd(d$perfam)
mean(d$perfam)-sd(d$perfam)
```

## Coding categorical variables

Contrast/effect/dummy coding can often be useful when including categorical variables as predictors in a general linear model. For a given variable, we can have $k-1$ contrasts (where $k$ is the number of levels). 

We've talked about 3 different types of coding:

* Dummy coding (0s and 1s, where one level is the baseline with all 0s)
* Effect coding (1s, 0s, -1s, where one level is the baseline with all -1s)
* Contrast coding (various numbers, but a contrast needs to sum to 1, and should be orthogonal with other contrasts)


### Converting from continuous to categorical variables
Let's say that we had predictor(s) that were categorical variables. For an example, we'll continue with this dataset, though note we would probably want to treat these variables as continuous rather than categorical (or at least as "ordinal") if doing research!

To split the`perfam` variable three groups, we'll use the `quantile` function to divide our variable `perfam`, the percentage of employees with families in the organization, into thirds (*note that we've specified our probabilities as .33 and .66 accordingly). We change it into a categorical variable using the `findInterval` function.  This will allow us to have a categorical variable with three equal groups. 

```{r quantile}
quantpf3 = quantile(d$perfam, probs = c(.34, .66)); print(quantpf3)   
d$perfamcat = findInterval(d$perfam, quantpf3)  		

table(d$perfamcat)
str(d)

d$PerCat <- factor(d$perfamcat, labels=c('low fam','middle fam', 'high fam'))
# table(d$PerCat)
```

Similarly, we'll convert `famprog` into a categorical variable using the median to split the variable into "low family programs" and "high family programs".
```{r}
quantfp2 = median(d$famprog)
print(quantfp2)
d$famprogcat = findInterval(d$famprog, quantfp2)

table(d$famprogcat)

d$FPcat <- factor(d$famprogcat, labels = c("lowprog", "highprog"))
```

```{r second look}
str(d) # note variables that are chr not factor! problem for contrasts
# contrasts(d$Fcat)=cbind(-1,1)
```

Let's plot our data! 

```{r plotting}
ggplot(d, aes(x=FPcat, y=empsatis, color=PerCat)) +
  geom_boxplot(alpha=.7)
```

How can we make bar graphs?  (Since we're using two categorical predictors)

We'll also add error bars! We'll include a formula for the standard error that you could use.

```{r formulas}
sem <- function(x) {sd(x) / sqrt(length(x))}
```

```{r plotting error bars}
ms <- aggregate(empsatis ~ FPcat + PerCat, data=d, mean)
ms$errs <- aggregate(empsatis ~ FPcat + PerCat, data=d, sem)$empsatis
print(ms)

ggplot(ms, aes(x=FPcat, y=empsatis, fill=PerCat)) + 
  geom_bar(position=position_dodge(), stat="identity", colour="black", size=.3) + # Use black outlines
  geom_errorbar(aes(ymin = ms$empsatis-ms$errs, ymax=ms$empsatis+ms$errs), width=.2, position=position_dodge(width=.9)) +
  xlab("Number of Programs") +
  ylab("Employee Satisfaction") +
  theme_bw()
```

### Generate a hypothesis

Contrasts can often be helpful because **you can make really specific predictions about trends in your data**. For instance, you might think (1) group A and group D should should a positive effect, and group B and group C might have a negative effect, and (2) that group B has a greater effect than group C, etc. Using an ANOVA, we can just test if the groups are different from each other, but not specific patterns of *how* they differ. However, using contrasts, we can look for specific patterns! Here, it's often useful (and recommended) that you generate a hypothesis for what pattern you would expect to see before making random contrasts and testing them!

Let's revisit our data, but just take a look at the companies that had a low number of programs, since that seemed to be where interesting things were happening!

```{r subsetting data}
l = subset(d, FPcat=='lowprog')
str(l)
```

What if we change our contrasts? What predictions might we make using the data?

```{r contrasts}
qplot(PerCat, empsatis, data = d, geom = "boxplot")
levels(l$PerCat)

# here' are the's the default coding:
contrasts(l$PerCat)

# now let's change it
contrasts(l$PerCat) = cbind(hiVSother = c(1,1,-2), lowVSmid = c(1,-1,0)) #which groups are these contrasts comparing?
contrasts(l$PerCat)

with(l, summary(lm(empsatis ~ PerCat)))
```

### Interpreting our contrasts

Here, our first contrast (hiVSother) compares "high fam" to the average of "low" and "middle fam". Our second contrast (which is nested in our first contrast) compares "low fam" to "middle fam". They are orthogonal because the products of the elements in the contrasts sums to zero ($1*1 + 1*-1 + -2*0 = 0$).


### Orthogonality

We've talked in class about contrasts that are orthogonal. In general, the elements in a contrasts should sum to zero (e.g., `sum(c(-1, 1)) = 0`), but in order to be orthogonal, the product of the elements in each contrast should sum to zero. In other words, for two contrasts $a$ and $b$, each of length $t$:

$$\sum\limits_{i=1}^t a_i b_i = 0$$

Why do we want to create orthogonal contrasts? 

```{r}
res = lm(empsatis ~ PerCat, data=l)
summary(res)

contrasts(l$PerCat)

designmat = data.frame(model.matrix(res))
head(l)
head(designmat)
```


Let's create some orthogonal and non-orthogonal contrasts and see which are which!

```{r}
c(-2,1,1) %*% c(0,1,-1)

c1 <- cbind(c(-2,1,1), c(0,1,-1)); sum(c1[,1] * c1[,2])
c2 <- cbind(c(-2,1,1), c(1,1,-2)); sum(c2[,1] * c2[,2])
c3 <- cbind(c(-2,1,1), c(1,1,-3)); sum(c3[,1] * c3[,2])
```


```{r}
c1 <- cbind(c(-3,1,1,1), c(0,0,1,-1), c(-1,1,1,-1)); c1
c2 <- cbind(c(1,1,-1,-1), c(1,-1,1,-1), c(-1,1,1,-1)); c2
```

Let's take a closer look at each of these contrasts to see what groups they would be comparing!

### More on contrasts

In this dataset the researchers were interested in the effect of vitamin C on tooth grown in guinea pigs. Our dataframe has info about tooth length (`len`), the supplement which was administered (`supp`: orange juice (OJ) or ascorbic acid (VC)), and the doseage in mg (`dose`), which we'll recode as "low", "med", and "high".
```{r}
data(ToothGrowth)
df = data.frame(ToothGrowth)
df$dose = factor(df$dose,
                 levels=c(0.5,1.0,2.0),
                 labels=c("low","med","high"))
str(df)
```

#### Effect coding
```{r}
contrasts(df$supp) = cbind(OJvsVC=c(1, -1)); contrasts(df$supp)
contrasts(df$dose) = cbind(C1=c(1, -1, 0), 
                           C2=c(0, -1, 1)); contrasts(df$dose)

summary(lm(len ~ supp + dose, data=df))

with(df, tapply(len, list(dose), mean)) # means by group
mean(df$len) #unweighted grand mean
```

Here, our intercept is our (unweighted) "grand mean" of tooth length (it's weighted if the cell counts are balanced). Then, for our dose constrasts, C1 indicates the difference of "low" from the grand mean ($18.8133-8.2083=10.6$), and C2 indicates the difference of "high" from the grand mean ($18.8133+7.2867=26.1$). Thus, we can say that those with low doses (collapsing across the supplement) had significantly less tooth growth than average [**interpretation of effect!**], $b=-8.21, t(56)=-11.75, p < 0.001$ [**stats!**]. This might be because vitamin C promotes tooth growth [**causal story!**]. However, those with high doses had more tooth growth than average, $b=7.29, t(56)=10.43, p < 0.01$.


Another way to look at the contrasts is as a weighted combination of the group means. Here, we have to take the inverse of R's contrast codes, and we get the weights for each group:
```{r}
tmp <- contrasts(df$dose)
tmp <- cbind(constant=1,tmp) # add an intercept (1s)
weights = solve(tmp); weights
```

**intercept estimate** = .333 * low + .333 * med + .333 * high

**C1 estimate** = .666 * low + -.333 * med + -.333 * high = low - (low + med + high)/3

**C2 estimate** = -.333 * low + -.333 * med + .666 * high = high - (low + med + high)/3

```{r}
means = with(df, tapply(len, list(dose), mean)) # means by group
means

sum(means * weights[1,]) # intercept
sum(means * weights[2,]) # C1
sum(means * weights[3,]) # C2
```


Now, let's turn to using **contrast coding** to test specific predictions! These should be decided *a priori*, and are thus often called "**planned comparisons**".

#### Contrast coding
```{r}
contrasts(df$supp) = cbind(OJvsVC=c(1, -1)); contrasts(df$supp)

contrasts(df$dose) = cbind(OTHERvsLOW=c(-2, 1, 1), 
                           HIGHvsMED=c(0, -1, 1)); contrasts(df$dose)
```

```{r}
res = lm(len ~ supp * dose, data=df)
anova(res)

# Compare with aov
summary(aov(len ~ supp * dose, data=df))
```

```{r}
summary(res)
```

Here, our doseOTHERvsLOW contrast shows us that the low dose group has significantly lower tooth growth than the other two groups, $b=4.10, t(54)=12.38, p < 0.01$. Further, the doseHIGHvsMED contrast shows us that the high dose group has more tooth growth than the medium group, $b=3.18, t(54)=5.54, p < 0.01.$

Let's try to recreate our group means using the output!

```{r}
contrasts(df$supp)
contrasts(df$dose)

OJ_low = 18.8133 + 1.8500*1 + 4.1042*-2 + 3.1825*0 + -0.3875*1*-2 + -1.5025*1*0
OJ_med = 18.8133 + 1.8500*1 + 4.1042*1 + 3.1825*-1 + -0.3875*1*1 + -1.5025*1*-1
OJ_high = 18.8133 + 1.8500*1 + 4.1042*1 + 3.1825*1 + -0.3875*1*1 + -1.5025*1*1

VC_low = 18.8133 + 1.8500*-1 + 4.1042*-2 + 3.1825*0 + -0.3875*-1*-2 + -1.5025*-1*0
VC_med = 18.8133 + 1.8500*-1 + 4.1042*1 + 3.1825*-1 + -0.3875*-1*1 + -1.5025*-1*-1
VC_high = 18.8133 + 1.8500*-1 + 4.1042*1 + 3.1825*1 + -0.3875*-1*1 + -1.5025*-1*1
```

Now we'll compare these predicted means with the actual means!
```{r}
rbind(OJ = c(OJ_low, OJ_med, OJ_high), VC = c(VC_low, VC_med, VC_high))

with(df, tapply(len, list(supp,dose), mean))
```


Since the contrasts are orthogonal, our predictors aren't correlated!
```{r}
designmat = data.frame(model.matrix(res))
cor(designmat)
```


Again, another way to look at the contrasts is as a weighted combination of the group means. Here, we have to take the inverse of R's contrast codes, and we get the weights for each group:
```{r}
tmp <- contrasts(df$dose)
tmp <- cbind(constant=1,tmp) # add an intercept (1s)
weights = solve(tmp); weights
```

**intercept estimate** = .333 * low + .333 * med + .333 * high

**OTHERvsLOW estimate** = -.333 * low + 0.167 * med + 0.167 * high

**HIGHvsMED estimate** = 0 * low + -.5 * med + .5 * high

```{r}
coef(res)

sum(means * weights[1,]) # intercept
sum(means * weights[2,]) # OTHERvsLOW
sum(means * weights[3,]) # HIGHvsMED
```

## Mediation (Question E)

Let's clean up our screens and load in the next data set!
```{r}
# rm(list=ls())

d = read.csv('http://www.stanford.edu/class/psych252/data/caffeine.csv')
str(d)
```

**Measures**

What mediational question might we ask with these data?

**IV**: *Coffee* - 20 subjects in each group either had 0 cups, 2 cups, or 4 cups

**DV**: *Performance* - on a stats quiz with 10 problems, 5-89 points

**Possible Mediator 1**: *Number of problems attempted* (hyperactivity)

**Possible Mediator 2**: *Accuracy* - how likely they were to get a problem right if they tried (better success)

What should we do first?

Let's visualize the data:
```{r}
plot_dataframe(d)
table(d$coffee)
```

We should probably recode coffee cups into number of coffee cups!

```{r}
summary(d)
d$cups = 0*as.numeric(d$coffee==1) + 2*as.numeric(d$coffee==2) + 4*as.numeric(d$coffee==3) 
table(d$cups)
```

```{r}
ggplot(d, 
       aes(x=cups, 
           y=perf, size=numprob)) +  # Adding color for mentill
  geom_point(shape=1, position=position_jitter(width=.5)) +  
  geom_smooth(method=lm, se=TRUE) +
  theme_bw()
```

First question: Does the number of problems attempted (hyperactivity) mediate the effect of coffee on performance?
What is our x (IV)?
Our y (DV)? Our mediator?

We need to run three models. There is one model that we never run (the effect of the mediator on the DV, without the IV included):
```{r}
with(d, summary(lm(perf~numprob)))
```

The first model we need to look at is the direct path, does coffee predict performance?  If not, we can abandon this whole endeavor!
```{r model 1}
problm1<-lm(perf~cups,data=d)
summary(problm1) # yes, it predicts, c=3.74
c<-problm1$coefficients[2]; c # We'll save our coefficients for our Sobel test later!

problm1<-lm(perf~coffee,data=d) # Note that we get the same results whether we recode coffee or not, just different coefficients
summary(problm1)
```

Now let's check out Model 2, whether the IV affects the mediator, in other words, does coffee predict the number of problems attempted?

```{r model 2}
problm2<-lm(numprob~cups,data=d)
summary(problm2) # yes, a=0.52
a<-summary(problm2)$coefficients[2,1]; a
s_a<-summary(problm2)$coefficients[2,2]; s_a
```

Our final model, Model 3, is the effect of coffee on performance mediated by the effect of the number of problems.
```{r model 3}
problm3<-lm(perf~cups+numprob,data=d)
summary(problm3) 

c_prime<-summary(problm3)$coefficients[2,1]; c_prime
b<-summary(problm3)$coefficients[3,1]; b
s_b<-summary(problm3)$coefficients[3,2]; s_b

# note, we can also get c_prime by subtracting a * b from our original c:
c_prime
c - a*b
```

The direct effect of coffee (c) disappeared and the number of problems attempted (b) is significant. We could answer yes, there is mediation, but let's be more formal.

Let's perform  the conventional Sobel test, adding in the standard error of a and standard error of b.
```{r sobel}
s_ab <- sqrt(b^2*s_a^2+a^2*s_b^2+s_a^2*s_b^2)
s_ab # standard error of a*b

a*b
a*b/s_ab

p_s_ab<-pnorm(a*b/s_ab, lower.tail=F)
p_s_ab # p of ratio of a*b over its s.e.
```

Now let's repeat the procedure for the second mediation analysis. The question now is: does accuracy meadiate the effect of coffee on performance? 

We did Model 1 above and have significant c (the direct path)

Now let's move on to Model 2, does coffee predict accuracy?

```{r second med}
accurm2<-lm(accur~cups,data=d)
summary(accurm2)
```

No, coffee consumption does not predict accuracy, so according to Baron & Kenny we can stop and conclude there is no mediation. But lets procede, using a=-0.00014.

```{r}
a2<-summary(accurm2)$coefficients[2,1]
s_a2<-summary(accurm2)$coefficients[2,2]
```

Now model 3, is the effect of coffee on performance mediated by the effect of accuracy?

```{r}
accurm3<-lm(perf~cups+accur,data=d)
summary(accurm3) # now the effect of coffee remains as well as an effect of accur.

b2<-summary(accurm3)$coefficients[3,1]
s_b2<-summary(accurm3)$coefficients[3,2]
```

Perform conventional sobel test, adding standard error of a and b.

```{r}
s_ab2 <- sqrt(b2^2*s_a2^2+a2^2*s_b2^2+s_a2^2*s_b2^2)
s_ab2 # standard error of a*b
p_s_ab2<-pnorm(a2*b2/s_ab2,lower.tail=F)
p_s_ab2 # p of ratio of a2*b2 over its s.e.
```

Conclusion: Coffee and accuracy both contribute to performance and in this case there is no mediation there. However, the effect of coffee is mediated by the number of problems attempted. 

## Bootstrapped Mediation

Using a modified version of Benoit's script, let's re-run the analysis from before:
```{r}
mediation_bootstrap = function(x, med, y, iterations = 1000){
  
  # setup some parameters
  N = length(x)
  df = as.data.frame(cbind(x, med, y))
  boot_ab = vector(length=iterations) # set up empty vector for storage
  
  # now go through a loop where we'll randomly sample, and get a a*b value
  for (i in 1:iterations){
    ind_boot = sample(c(1:N), N, replace=TRUE) # random indices
    df_boot = df[ind_boot,]
      
    iter_a = lm(df_boot$med ~ df_boot$x)$coefficients[2] # coeff of x
    iter_b = lm(df_boot$y ~ df_boot$med + df_boot$x)$coefficients[2] # coeff of mediator
    
    boot_ab[i] = iter_a * iter_b
  }
  
  # create plot
  hist(boot_ab,main=paste("Bootstrapped a*b, with",iterations,"iterations"),col="red");
  abline(v=0, col='black', lty=2, lwd=2)
  abline(v=c(quantile(boot_ab,c(.025,.975))), col='blue', lty=3)
  
  # Print results
  print("Bootstrap results:",quote=F);
  print(c(ab=mean(boot_ab)));
  print(quantile(boot_ab,c(.025,.975)))
  
  return(boot_ab)
}
```

```{r}
boot_ab = mediation_bootstrap(x=d$cups, med=d$numprob, y=d$perf, iterations=10000)
mean(boot_ab)

# compared to our ab from before:
a*b
```
